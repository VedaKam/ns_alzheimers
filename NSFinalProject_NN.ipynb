{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL PROJECT\n",
    "Dora John and Veda Kamaraju"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pdb import set_trace as st\n",
    "from copy import copy\n",
    "import seaborn as sns\n",
    "import fastcluster\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from scipy import stats\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_matrix = 'ExpressionData.csv'\n",
    "\n",
    "mRNA_top = pd.read_csv(expression_matrix, delimiter='\\t', skiprows=[0,1], nrows=5)\n",
    "\n",
    "# Annotations File\n",
    "\n",
    "Annotations_File = 'AnnotationsFile.csv'\n",
    "annotations = pd.read_csv(Annotations_File, delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entorhinal cortex', 'hippocampus', 'middle temporal gyrus', 'posterior cingulate cortex', 'primary visual cortex', 'superior frontal gyrus']\n"
     ]
    }
   ],
   "source": [
    "region_col = 'Sample Characteristic[organism part]'\n",
    "regions = annotations[region_col]\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "region_col = 'Sample Characteristic[organism part]'\n",
    "unique_regions = sorted(annotations[region_col].unique())\n",
    "print(unique_regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['GSM119653', 'GSM119629', 'GSM238802', 'GSM238953', 'GSM119687',\n",
      "       'GSM119647', 'GSM119643', 'GSM119660', 'GSM238837', 'GSM238942',\n",
      "       ...\n",
      "       'GSM119662', 'GSM119635', 'GSM119625', 'GSM119651', 'GSM119646',\n",
      "       'GSM238870', 'GSM238839', 'GSM119688', 'GSM238815', 'GSM119683'],\n",
      "      dtype='object', length=161)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib import colors as mcolors\n",
    "from copy import copy\n",
    "\n",
    "expr = pd.read_csv('ExpressionData.csv', delimiter=',')\n",
    "expr = expr.rename(columns={expr.columns[0]: 'Gene_ID'})\n",
    "\n",
    "expr.set_index('Gene_ID', inplace=True)\n",
    "\n",
    "data = copy(expr.iloc[:, 2:])\n",
    "\n",
    "# Drop rows with NaN or 0 values\n",
    "data = data.dropna()\n",
    "data = data[(data != 0).all(axis=1)]\n",
    "\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "\n",
    "data_row_norm = data.div(data.mean(axis=1), axis=0)\n",
    "data_col_norm = data_row_norm.div(data_row_norm.mean(axis=1), axis=0)\n",
    "\n",
    "# Log2 transform\n",
    "data_log2 = np.log2(data_col_norm)\n",
    "print(data_log2.columns)\n",
    "\n",
    "\n",
    "metadata = pd.DataFrame({\n",
    "    'Sample': data_log2.columns,\n",
    "    'Region': annotations['Sample Characteristic[organism part]'].values,\n",
    "    'Disease': annotations['Sample Characteristic[disease]'].values,\n",
    "    'Gender': annotations['Sample Characteristic[sex]'].values,\n",
    "})\n",
    "\n",
    "metadata['Individual'] = np.arange(1, len(metadata)+1)\n",
    "\n",
    "\n",
    "metadata.set_index('Sample', inplace=True)\n",
    "\n",
    "top_genes = data_log2.var(axis=1).sort_values(ascending=False).head(200).index\n",
    "data_top = data_log2.loc[top_genes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this portion of the code, the dataset is loaded for gene expression and sample metadata. Initially, the code filters and normalizes the data. Following, top variable genes are selected and the data is prepared for binary classification of those samples which are Alzheimer's or control, pulling their respective gene expressions. A neural network is then trained to predict disease status from expression and test accuracy is reported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.0000\n",
      "Epoch 2/30, Loss: 0.0000\n",
      "Epoch 3/30, Loss: 0.0000\n",
      "Epoch 4/30, Loss: 0.0000\n",
      "Epoch 5/30, Loss: 0.0000\n",
      "Epoch 6/30, Loss: 0.0000\n",
      "Epoch 7/30, Loss: 0.0000\n",
      "Epoch 8/30, Loss: 0.0000\n",
      "Epoch 9/30, Loss: 0.0000\n",
      "Epoch 10/30, Loss: 0.0000\n",
      "Epoch 11/30, Loss: 0.0000\n",
      "Epoch 12/30, Loss: 0.0000\n",
      "Epoch 13/30, Loss: 0.0000\n",
      "Epoch 14/30, Loss: 0.0000\n",
      "Epoch 15/30, Loss: 0.0000\n",
      "Epoch 16/30, Loss: 0.0000\n",
      "Epoch 17/30, Loss: 0.0000\n",
      "Epoch 18/30, Loss: 0.0000\n",
      "Epoch 19/30, Loss: 0.0000\n",
      "Epoch 20/30, Loss: 0.0000\n",
      "Epoch 21/30, Loss: 0.0000\n",
      "Epoch 22/30, Loss: 0.0000\n",
      "Epoch 23/30, Loss: 0.0000\n",
      "Epoch 24/30, Loss: 0.0000\n",
      "Epoch 25/30, Loss: 0.0000\n",
      "Epoch 26/30, Loss: 0.0000\n",
      "Epoch 27/30, Loss: 0.0000\n",
      "Epoch 28/30, Loss: 0.0000\n",
      "Epoch 29/30, Loss: 0.0000\n",
      "Epoch 30/30, Loss: 0.0000\n",
      "Test Accuracy: 57.58%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = data_top.T.values  \n",
    "y_labels = metadata.loc[data_top.columns, 'Disease'].values \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_labels)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# SPLIT DATA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class FullyConnectedNN1(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = FullyConnectedNN1(X_train, y_train)\n",
    "test_dataset = FullyConnectedNN1(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "class FullyConnectedNN1(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FullyConnectedNN1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model = FullyConnectedNN1(input_dim=X_train.shape[1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accruacy:  0.7589285714285714\n",
      "Validation Accuracy:  0.7755102040816326\n",
      "Matched genes: ['MDH1', 'IDH3G', 'DLD', 'FH', 'ACO2', 'PCK2', 'IDH3B', 'OGDH', 'SDHB', 'DLST', 'ACO1', 'PCK1', 'ACLY', 'PDHA1', 'SUCLA2', 'IDH1', 'MDH2', 'DLAT', 'PDHA2', 'SUCLG1', 'IDH3A', 'PDHB', 'SUCLG2', 'PC', 'IDH2', 'OGDHL', 'SDHD']\n",
      "Number of matched genes: 27\n",
      "Predicted class for GSM119687 : Alzheimers disease\n"
     ]
    }
   ],
   "source": [
    "# TCA CYCLE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# HAVE TO LOAD IT AGAIN AND CLEAN IT UP TO MAP TO TCA FILE\n",
    "\n",
    "expression_data = pd.read_csv(\"ExpressionData.csv\")\n",
    "expression_data_clean = expression_data.drop(columns=['Gene ID', 'DesignElementAccession'])\n",
    "expression_data_clean.set_index('Gene Name', inplace=True)\n",
    "expression_data_clean = expression_data_clean.apply(pd.to_numeric, errors='coerce')\n",
    "data_top = np.log2(expression_data_clean + 1)\n",
    "\n",
    "def GetPathway(FilePath):\n",
    "    with open(FilePath, 'r') as file:\n",
    "        matches = re.findall(r'\\b([A-Z0-9]+);', file.read())\n",
    "    return np.array(matches)\n",
    "\n",
    "Pathway_Genes = np.char.strip(GetPathway(\"tca.txt\"))\n",
    "Pathway_Data = data_top.loc[data_top.index.intersection(Pathway_Genes)]\n",
    "\n",
    "metadata = pd.read_csv(\"AnnotationsFile.csv\")\n",
    "metadata.set_index(\"Assay\", inplace=True)\n",
    "sample_labels = metadata.loc[Pathway_Data.columns, 'Sample Characteristic[disease]'].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(sample_labels)\n",
    "label_to_int = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "int_to_label = {v: k for k, v in label_to_int.items()}\n",
    "\n",
    "train_cols, val_cols, train_labels, val_labels = train_test_split(\n",
    "    Pathway_Data.columns,\n",
    "    encoded_labels,\n",
    "    test_size=0.3,\n",
    "    stratify=encoded_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "class TranscriptomicsDataset(Dataset):\n",
    "    def __init__(self, data, columns, labels):\n",
    "        self.data = data[columns].T\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.data.iloc[idx].values, dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return sample, label\n",
    "\n",
    "train_dataset = TranscriptomicsDataset(Pathway_Data, train_cols, train_labels)\n",
    "val_dataset = TranscriptomicsDataset(Pathway_Data, val_cols, val_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "class FullyConnectedNN2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(FullyConnectedNN2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "input_size = len(Pathway_Data.index)\n",
    "model = FullyConnectedNN2(input_size, 16, 8, 2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def evaluate(loader):\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct_train / total_train\n",
    "    val_loss, val_acc = evaluate(val_loader)\n",
    "\n",
    "print(\"Training Accruacy: \", train_acc)\n",
    "print(\"Validation Accuracy: \", val_acc)\n",
    "\n",
    "torch.save(model.state_dict(), 'trained_model.pth')\n",
    "\n",
    "trained_model = FullyConnectedNN2(input_size, 16, 8, 2)\n",
    "trained_model.load_state_dict(torch.load(\"trained_model.pth\"))\n",
    "trained_model.eval()\n",
    "\n",
    "print(\"Matched genes:\", list(Pathway_Data.index))\n",
    "print(\"Number of matched genes:\", len(Pathway_Data))\n",
    "\n",
    "# TEST EXAMPLE \n",
    "\n",
    "sample_id = 'GSM119687'\n",
    "Test_Array = torch.from_numpy(Pathway_Data.loc[:, sample_id].to_numpy()).float().unsqueeze(0)\n",
    "output = trained_model(Test_Array)\n",
    "_, predicted_class = torch.max(output, 1)\n",
    "predicted_class_name = int_to_label[predicted_class.item()]\n",
    "print(\"Predicted class for\", sample_id, \":\", predicted_class_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this portion of the code, initially there are a sequence of imports, metadata is loaded about the samples, and organ/tissue region is pulled out. The entire expression matrix is read and gene identifiers are set as the DataFrame index. The data is log-transformed and a sample metadata table is constructed. The top 200 most variable genes across all samples are selected for analysis. To prepare the data for classification, the gene matrix is transposed to a shape and disease status is encoded as 0 or 1; the data is split into testing/training sets. The PyTorch dataset is then used, which wraps the dataset into loaders. The fully connected neural network is then established, with a training loop for a loss function (utilizing the Adam optimizer), and training is run for 30 epochs (includes forward pass, loss computation, backpropagation, and updated weights). Lastly, the model was evaluated on a test set which revealed a 76% testing accuracy and 78% validation accuracy. \n",
    "\n",
    "Our training model ultimately classified tissue as Alzheimers disease or control based on the expression of TCA cycle genes which are implicated in energy metabolism. The model correctly classified 76% of the training samples and learned the pattern in the training data. It also predicted 78% of the unseen samples -- validation accuracy being slightly higher in the training shows that there is no overfitting and the model generalizes well. There are about 27 genes indicated as being matched to an unknown gene list. In attempting to match model-selected genes to known biology like altered energy metabolis, this is predictive of strong biological interpretability. IDH3A, PDHA1, and OGDH are all mitochondrial/TCA-cycel related, whose alteration in expression makes sense to be downregulated in Alzheimer's disease (decreased metabolism). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ns4300",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
